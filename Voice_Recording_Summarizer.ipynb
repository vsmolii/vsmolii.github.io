{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsmolii/vsmolii.github.io/blob/master/Voice_Recording_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Configuration"
      ],
      "metadata": {
        "id": "W7YFzJxfdVxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: connect Google Drive"
      ],
      "metadata": {
        "id": "1Ch9UKUuc5pF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content')"
      ],
      "metadata": {
        "id": "aqgazEzVveFg",
        "outputId": "ff8ab861-c44a-47ca-dfee-7d5f40dadc67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure the variables"
      ],
      "metadata": {
        "id": "rvQAeOSZcsJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whisper_model_size = 'medium'\n",
        "# whisper_model_size = 'large-v2'\n",
        "\n",
        "\n",
        "language = 'uk' # Ukrainian\n",
        "# language = 'en' # English\n",
        "# language = 'be' # Belarusian\n",
        "# language = 'bg' # Bulgarian\n",
        "# language = 'hr' # Croatian\n",
        "# language = 'cs' # Czech\n",
        "# language = 'da' # Danish\n",
        "# language = 'nl' # Dutch\n",
        "# language = 'et' # Estonian\n",
        "# language = 'fi' # Finnish\n",
        "# language = 'fr' # French\n",
        "# language = 'de' # German\n",
        "# language = 'el' # Greek\n",
        "# language = 'hu' # Hungarian\n",
        "# language = 'is' # Icelandic\n",
        "# language = 'it' # Italian\n",
        "# language = 'lv' # Latvian\n",
        "# language = 'lt' # Lithuanian\n",
        "# language = 'no' # Norwegian\n",
        "# language = 'pl' # Polish\n",
        "# language = 'pt' # Portuguese\n",
        "# language = 'ro' # Romanian\n",
        "# language = 'ru' # Russian\n",
        "# language = 'sr' # Serbian\n",
        "# language = 'sk' # Slovak\n",
        "# language = 'sl' # Slovenian\n",
        "# language = 'es' # Spanish\n",
        "# language = 'sv' # Swedish\n",
        "# language = 'tr' # Turkish\n",
        "\n",
        "content_source = \"youtube\"\n",
        "# content_source = \"file\"\n",
        "# content_source = \"google_drive\"\n",
        "\n",
        "## Youtube url is use\n",
        "# youtube_url = \"https://www.youtube.com/watch?v=qowrj7JbeZs\"\n",
        "youtube_url = \"https://www.youtube.com/watch?v=JpFQkCMRQYc\"\n",
        "\n",
        "# file_path = \"/content/audio_transcript.txt\"\n",
        "# file_path = \"/content/drive/MyDrive/Meet Recordings/orq-twiv-trk (2023-08-30 15:39 GMT+1)\" # 56 min mp4 video, 463 MB\n",
        "# file_path = \"/content/drive/MyDrive/Meet Recordings/wic-idnb-rfp (2023-08-31 13:12 GMT+1)\" # 20 min mp4 video, 34 MB\n",
        "# file_path = \"/content/orq-twiv-trk (2023-08-30 15:39 GMT+1).mp3\" # 56 min mp3 audio\n",
        "\n",
        "## This will be used in the summarizing prompt:\n",
        "# recording_type = \"meeting\"\n",
        "# recording_type = \"user interview\"\n",
        "recording_type = \"youtube video\"\n",
        "\n",
        "## Meeting summary:\n",
        "# summary_structure = \"\"\"\n",
        "  # ## Summary\n",
        "  # 2-3 sentences\n",
        "\n",
        "  # ## Topics\n",
        "  # - one\n",
        "  # - two\n",
        "  # - ..\n",
        "\n",
        "  # ## Decisions\n",
        "  # - one\n",
        "  # - two\n",
        "  # - ..\n",
        "\n",
        "  # ## Action items\n",
        "  # - one\n",
        "  # - two\n",
        "  # - ..\n",
        "# \"\"\"\n",
        "\n",
        "# Generic summary:\n",
        "summary_structure = \"\"\"\n",
        "  ## Summary\n",
        "  2-3 sentences\n",
        "\n",
        "  ## Highlights\n",
        "  - one\n",
        "  - two\n",
        "  - ..\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "## User interview summary:\n",
        "# summary_structure = \"\"\"\n",
        "#   ## Summary\n",
        "#   Main highlights here. 3-7 paragraphs.\n",
        "\n",
        "#   ## Jobs\n",
        "#   - one\n",
        "#   - two\n",
        "#   - ..\n",
        "\n",
        "#   ## Pains\n",
        "#   - one\n",
        "#   - two\n",
        "#   - ..\n",
        "\n",
        "#   ## Gains\n",
        "#   - one\n",
        "#   - two\n",
        "#   - ..\n",
        "# \"\"\"\n"
      ],
      "metadata": {
        "id": "kPvkLvp5cMNd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing the dependencies"
      ],
      "metadata": {
        "id": "qfNOHKhmcz8G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JS2OUAYCbKZV"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git  -q\n",
        "!pip install langchain moviepy openai tiktoken pytube"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enter your OpenAI API key"
      ],
      "metadata": {
        "id": "PwEFMHWNeoJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "openai.api_key = getpass('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')"
      ],
      "metadata": {
        "id": "2JLDNhTmfXpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: check if the API key is valid by listing models"
      ],
      "metadata": {
        "id": "bPyvuYfuexw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can confirm that the API key works by listing all the OpenAI models\n",
        "models = openai.Model.list()\n",
        "for model in models[\"data\"]:\n",
        "  print (model[\"root\"])"
      ],
      "metadata": {
        "id": "Y3pxHQNofZ_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the Whisper model (only once)"
      ],
      "metadata": {
        "id": "O5FE2QgbdhKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import pathlib\n",
        "import whisper\n",
        "\n",
        "model_path = pathlib.Path(\"/content/whisper/\"+whisper_model_size+\".pt\")\n",
        "if model_path.exists():\n",
        "  print (\"Model has been downloaded, no re-download necessary\")\n",
        "else:\n",
        "  print (\"Starting download of Whisper Model\")\n",
        "  whisper._download(whisper._MODELS[whisper_model_size], '/content/whisper/', False)"
      ],
      "metadata": {
        "id": "BdZpbojgbRaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Transcribe the recording\n",
        "- The video file will be automatically converted into audio before the transcription.\n",
        "- Text file will be immediately added as a transcription"
      ],
      "metadata": {
        "id": "-aYrcnYAe-WY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import *\n",
        "import os\n",
        "from pytube import YouTube\n",
        "text_trascript_file_path = False\n",
        "\n",
        "def download_audio_from_youtube(youtube_url):\n",
        "  print (\"Ok, a youtube link. Converting to audio file...\")\n",
        "  output_path='/content/youtube_audio/'\n",
        "  yt = YouTube(youtube_url)\n",
        "  audio_stream = yt.streams.filter(only_audio=True, file_extension='mp4').first()\n",
        "  download_location = audio_stream.download(output_path)\n",
        "  new_location = download_location.replace('.mp4', '.mp3')\n",
        "  os.rename(download_location, new_location)\n",
        "  file_path = os.path.abspath(new_location)\n",
        "  print (\"All done. Saved to \"+file_path)\n",
        "  return file_path\n",
        "\n",
        "def transcribe_recording(whisper_model_size, file_path):\n",
        "  # if the file is a video, convert it to the .mp3 audio\n",
        "  if not file_path.endswith('.mp3'):\n",
        "    print (\"File is not an mp3. Converting to audio...\")\n",
        "    video = VideoFileClip(file_path)\n",
        "    base_name = os.path.basename(file_path)\n",
        "    output_name = os.path.splitext(base_name)[0] + \".mp3\"\n",
        "    output_path = os.path.join(\"/content/\", output_name)\n",
        "    video.audio.write_audiofile(output_path)\n",
        "    file_path = output_path\n",
        "  # load the whisper model\n",
        "  print (\"Starting the audio transcription...\")\n",
        "  whisper_model = whisper.load_model(whisper_model_size, device='cuda', download_root='/content/whisper/')\n",
        "  print (\"Loaded the '\"+whisper_model_size+\"' Whisper model...\")\n",
        "  result = whisper_model.transcribe(file_path, language=language)\n",
        "  return result['text']\n",
        "\n",
        "\n",
        "if content_source == \"youtube\":\n",
        "  file_path = download_audio_from_youtube(youtube_url)\n",
        "\n",
        "if file_path.endswith('.txt'):\n",
        "  print (\"The submitted file is a text file.\")\n",
        "  text_trascript_file_path = file_path\n",
        "  with open(text_trascript_file_path, \"r\") as file:\n",
        "    interview_transcript = file.read()\n",
        "else:\n",
        "  print (\"File is not a text. Parsing the media...\")\n",
        "  interview_transcript = transcribe_recording(whisper_model_size, file_path)\n",
        "  base_name = os.path.basename(file_path)\n",
        "  text_output_name = \"transcript__\"+os.path.splitext(base_name)[0] + \".txt\"\n",
        "  print (\"The transcript is ready. Saving as a \"+text_output_name+\" ...\")\n",
        "  with open(text_output_name, \"w\") as file:\n",
        "      file.write(interview_transcript)\n",
        "  text_trascript_file_path = \"/content/\" + text_output_name\n",
        "\n",
        "import tiktoken\n",
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo-16k\")\n",
        "print (\"Number of tokens:\", len(enc.encode(interview_transcript)))"
      ],
      "metadata": {
        "id": "3CjenIR6zvf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the transcription happened correctly by peeking into the first 1000 characters\n",
        "interview_transcript[:1000]"
      ],
      "metadata": {
        "id": "kwPd3KEFfFbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Summarize the transcription\n",
        "\n",
        "The script will automatically choose the summarization method depending on the size of transcript:\n",
        "- gpt-4 for less than 8000 tokens\n",
        "- gpt-3.5-turbo-16k for less than 16000 tokens\n",
        "- map-reduce langchain method for more than 16000 tokens"
      ],
      "metadata": {
        "id": "dwrZy_Bq-OcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.mapreduce import MapReduceChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain, StuffDocumentsChain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "def summarize_big_text(interview_transcript, max_tokens, model, recording_type, summary_structure):\n",
        "\n",
        "\n",
        "  map_template = \"\"\"The following is a set of \"\"\"+recording_type+\"\"\" recordings of a single \"\"\"+recording_type+\"\"\".\n",
        "    {docs}\n",
        "    Based on this list of recordings, please identify the main themes.\n",
        "\n",
        "    Structure the summary as following:\n",
        "\n",
        "    \"\"\"+summary_structure+\"\"\"\n",
        "\n",
        "    Helpful Answer:\"\"\"\n",
        "\n",
        "  reduce_template = \"\"\"The following is set of summaries of parts of a \"\"\"+recording_type+\"\"\":\n",
        "    {doc_summaries}\n",
        "    Take these and distill it into a final, consolidated summary.\n",
        "\n",
        "    Structure the summary as following:\n",
        "\n",
        "    \"\"\"+summary_structure+\"\"\"\n",
        "\n",
        "    Helpful Answer:\"\"\"\n",
        "\n",
        "  def break_into_chunks(text, max_tokens):\n",
        "    return RecursiveCharacterTextSplitter(\n",
        "        chunk_size = max_tokens,\n",
        "        chunk_overlap  = 0,\n",
        "        length_function = len,\n",
        "        add_start_index = True,\n",
        "    ).create_documents([text])\n",
        "\n",
        "  docs = break_into_chunks(interview_transcript, max_tokens)\n",
        "\n",
        "  for idx, chunk in enumerate(docs, 1):\n",
        "      print(f\"Chunk {idx} has {len(tiktoken.encoding_for_model('gpt-3.5-turbo-16k').encode(chunk.page_content))} tokens\")\n",
        "\n",
        "  llm = ChatOpenAI(temperature=0, openai_api_key=openai.api_key, model_name=model)\n",
        "  # Map\n",
        "\n",
        "  map_prompt = PromptTemplate.from_template(map_template)\n",
        "  map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
        "\n",
        "  # Reduce\n",
        "\n",
        "  reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
        "\n",
        "  # Run chain\n",
        "  reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "  # Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
        "  combine_documents_chain = StuffDocumentsChain(\n",
        "      llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
        "  )\n",
        "\n",
        "  # Combines and iteravely reduces the mapped documents\n",
        "  reduce_documents_chain = ReduceDocumentsChain(\n",
        "      # This is final chain that is called.\n",
        "      combine_documents_chain=combine_documents_chain,\n",
        "      # If documents exceed context for `StuffDocumentsChain`\n",
        "      collapse_documents_chain=combine_documents_chain,\n",
        "      # The maximum number of tokens to group documents into.\n",
        "      token_max=max_tokens,\n",
        "  )\n",
        "\n",
        "  # Combining documents by mapping a chain over them, then combining results\n",
        "  map_reduce_chain = MapReduceDocumentsChain(\n",
        "      # Map chain\n",
        "      llm_chain=map_chain,\n",
        "      # Reduce chain\n",
        "      reduce_documents_chain=reduce_documents_chain,\n",
        "      # The variable name in the llm_chain to put the documents in\n",
        "      document_variable_name=\"docs\",\n",
        "      # Return the results of the map steps in the output\n",
        "      return_intermediate_steps=False,\n",
        "  )\n",
        "\n",
        "  text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "      chunk_size=1000, chunk_overlap=0\n",
        "  )\n",
        "  split_docs = text_splitter.split_documents(docs)\n",
        "  return map_reduce_chain.run(split_docs)\n",
        "\n",
        "def summarize_single_shot(interview_transcript, model, recording_type, summary_structure):\n",
        "  instructPrompt = \"\"\"\n",
        "  Instructions:\n",
        "  Summarize the following \"\"\"+recording_type+\"\"\" text into the \"\"\"+recording_type+\"\"\" notes. Respond with markdown.\n",
        "\n",
        "  Structure the summary as following:\n",
        "\n",
        "  \"\"\"+summary_structure+\"\"\"\n",
        "\n",
        "  Text:\n",
        "  \"\"\"\n",
        "\n",
        "  request = instructPrompt + interview_transcript\n",
        "\n",
        "  llm = ChatOpenAI(temperature=0, openai_api_key=openai.api_key, model_name=model)\n",
        "  chatOutput = llm([HumanMessage(content=request)])\n",
        "  return chatOutput.content\n",
        "\n",
        "target_model = \"gpt-3.5-turbo-16k\"\n",
        "chunk_max_tokens = 16000\n",
        "\n",
        "actual_tokens = len(tiktoken.encoding_for_model(target_model).encode(interview_transcript))\n",
        "print (\"Number of tokens in input prompt \", actual_tokens)\n",
        "\n",
        "if actual_tokens < 8000:\n",
        "  print (\"Number of tokens is less than 8000, using gpt-4...\")\n",
        "  target_model = \"gpt-4\"\n",
        "  summary_result = summarize_single_shot(interview_transcript, target_model, recording_type, summary_structure )\n",
        "elif actual_tokens < 16000:\n",
        "  print (\"Number of tokens is less than 16000, using gpt-3.5-16k...\")\n",
        "  summary_result = summarize_single_shot(interview_transcript, target_model, recording_type, summary_structure )\n",
        "else:\n",
        "  print (\"Number of tokens is more than 16000, breaking into chunks...\")\n",
        "  summary_result = summarize_big_text(interview_transcript, chunk_max_tokens, target_model, recording_type, summary_structure)\n",
        "\n",
        "print(summary_result)\n",
        "\n",
        "summary_result_output_name = \"summary__\"+os.path.splitext(base_name)[0] + \".txt\"\n",
        "with open(summary_result_output_name, \"w\") as file:\n",
        "    file.write(summary_result)\n",
        "\n"
      ],
      "metadata": {
        "id": "-geeeF4KNKY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: write results to a .txt file"
      ],
      "metadata": {
        "id": "-Y0hiYGGtklw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_result_output_name = \"summary__\"+os.path.splitext(base_name)[0] + \".txt\"\n",
        "with open(summary_result_output_name, \"w\") as file:\n",
        "    file.write(summary_result)"
      ],
      "metadata": {
        "id": "Gd0x7lDAgzgk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}